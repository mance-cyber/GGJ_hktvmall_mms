# å¤–éƒ¨æŠ“å–åŠŸèƒ½å®Œæ•´æ•™å­¸

> HKTVmall AI ç‡Ÿé‹ç³»çµ± - ç«¶çˆ­å°æ‰‹å•†å“æ•¸æ“šæŠ“å–æŒ‡å—

---

## ğŸ“‘ ç›®éŒ„

1. [å¿«é€Ÿé–‹å§‹](#å¿«é€Ÿé–‹å§‹)
2. [ç’°å¢ƒæº–å‚™](#ç’°å¢ƒæº–å‚™)
3. [Firecrawl API è¨­å®š](#firecrawl-api-è¨­å®š)
4. [æŠ“å–åŠŸèƒ½ä½¿ç”¨æ–¹å¼](#æŠ“å–åŠŸèƒ½ä½¿ç”¨æ–¹å¼)
5. [å¯¦æˆ°æ¡ˆä¾‹](#å¯¦æˆ°æ¡ˆä¾‹)
6. [é€²éšæŠ€å·§](#é€²éšæŠ€å·§)
7. [å¸¸è¦‹å•é¡Œæ’æŸ¥](#å¸¸è¦‹å•é¡Œæ’æŸ¥)

---

## å¿«é€Ÿé–‹å§‹

### 5 åˆ†é˜å¿«é€Ÿæ¸¬è©¦

```bash
# 1. é€²å…¥å¾Œç«¯ç›®éŒ„
cd backend

# 2. å•Ÿå‹•è™›æ“¬ç’°å¢ƒ
.\venv\Scripts\activate  # Windows
# source venv/bin/activate  # Mac/Linux

# 3. å•Ÿå‹• Python äº’å‹•ç’°å¢ƒ
python

# 4. æ¸¬è©¦æŠ“å–åŠŸèƒ½
>>> from app.connectors.firecrawl import get_firecrawl_connector
>>> connector = get_firecrawl_connector()
>>> info = connector.extract_product_info("https://example.com/product")
>>> print(info.name, info.price)
```

---

## ç’°å¢ƒæº–å‚™

### 1. æª¢æŸ¥ä¾è³´å®‰è£

```bash
# æŸ¥çœ‹å·²å®‰è£çš„å¥—ä»¶
pip list | grep firecrawl

# æ‡‰è©²çœ‹åˆ°ï¼š
# firecrawl-py    0.0.16
```

å¦‚æœæ²’æœ‰å®‰è£ï¼š

```bash
pip install firecrawl-py==0.0.16
```

### 2. æª¢æŸ¥ç’°å¢ƒè®Šæ•¸

åœ¨ `backend/.env` æ–‡ä»¶ä¸­ç¢ºèªä»¥ä¸‹é…ç½®ï¼š

```env
# Firecrawl API
FIRECRAWL_API_KEY=fc-YOUR-API-KEY-HERE

# æ•¸æ“šåº«ï¼ˆå¿…éœ€ï¼‰
DATABASE_URL=postgresql+asyncpg://user:password@localhost/dbname

# Redisï¼ˆç”¨æ–¼ Celery ä»»å‹™ï¼‰
REDIS_URL=redis://localhost:6379/0
CELERY_BROKER_URL=redis://localhost:6379/0
```

---

## Firecrawl API è¨­å®š

### æ­¥é©Ÿ 1ï¼šè¨»å†Š Firecrawl å¸³è™Ÿ

1. å‰å¾€ [https://firecrawl.dev](https://firecrawl.dev)
2. é»æ“Šã€ŒSign Upã€è¨»å†Šå¸³è™Ÿ
3. é©—è­‰éƒµç®±

### æ­¥é©Ÿ 2ï¼šç²å– API Key

1. ç™»å…¥å¾Œå‰å¾€ Dashboard
2. é»æ“Šã€ŒAPI Keysã€
3. é»æ“Šã€ŒCreate New Keyã€
4. è¤‡è£½ç”Ÿæˆçš„ Keyï¼ˆæ ¼å¼ï¼š`fc-xxxxxxxxxx`ï¼‰

### æ­¥é©Ÿ 3ï¼šé…ç½® API Key

**æ–¹æ³• Aï¼šé€é .env æ–‡ä»¶**ï¼ˆæ¨è–¦ï¼‰

```bash
# ç·¨è¼¯ backend/.env
FIRECRAWL_API_KEY=fc-YOUR-ACTUAL-API-KEY
```

**æ–¹æ³• Bï¼šé€éç’°å¢ƒè®Šæ•¸**

```bash
# Windows PowerShell
$env:FIRECRAWL_API_KEY="fc-YOUR-ACTUAL-API-KEY"

# Windows CMD
set FIRECRAWL_API_KEY=fc-YOUR-ACTUAL-API-KEY

# Mac/Linux
export FIRECRAWL_API_KEY=fc-YOUR-ACTUAL-API-KEY
```

### æ­¥é©Ÿ 4ï¼šé©—è­‰é…ç½®

```python
# åœ¨ Python ç’°å¢ƒä¸­æ¸¬è©¦
from app.config import get_settings

settings = get_settings()
print(f"API Key å·²è¨­å®š: {settings.firecrawl_api_key[:10]}...")
# æ‡‰è©²è¼¸å‡ºï¼šAPI Key å·²è¨­å®š: fc-abc1234...
```

---

## æŠ“å–åŠŸèƒ½ä½¿ç”¨æ–¹å¼

### æ–¹å¼ 1ï¼šç›´æ¥ä½¿ç”¨ Python ä»£ç¢¼

#### 1.1 åŸºæœ¬æŠ“å– - å–®å€‹å•†å“

```python
from app.connectors.firecrawl import get_firecrawl_connector

# ç²å–é€£æ¥å™¨å¯¦ä¾‹
connector = get_firecrawl_connector()

# æŠ“å–å•†å“è³‡è¨Š
url = "https://www.example-shop.com/product/12345"
product_info = connector.extract_product_info(url)

# æŸ¥çœ‹çµæœ
print(f"å•†å“åç¨±: {product_info.name}")
print(f"åƒ¹æ ¼: HK${product_info.price}")
print(f"åŸåƒ¹: HK${product_info.original_price}")
print(f"æŠ˜æ‰£: {product_info.discount_percent}%")
print(f"åº«å­˜ç‹€æ…‹: {product_info.stock_status}")
print(f"è©•åˆ†: {product_info.rating} ({product_info.review_count} è©•è«–)")
print(f"åœ–ç‰‡: {product_info.image_url}")
```

#### 1.2 æŠ“å–å‹•æ…‹é é¢

æŸäº›ç¶²ç«™ä½¿ç”¨ JavaScript å‹•æ…‹è¼‰å…¥å…§å®¹ï¼Œéœ€è¦ä½¿ç”¨ Actionsï¼š

```python
# è™•ç†éœ€è¦æ»¾å‹•æ‰èƒ½çœ‹åˆ°åƒ¹æ ¼çš„é é¢
product_info = connector.extract_product_info(
    url="https://dynamic-shop.com/product/999",
    use_actions=True  # å•Ÿç”¨å‹•æ…‹è™•ç†
)
```

#### 1.3 æ‰¹é‡ç™¼ç¾å•†å“ URL

```python
# ç™¼ç¾ç¶²ç«™ä¸Šæ‰€æœ‰å•†å“é é¢
base_url = "https://www.competitor-shop.com"
product_urls = connector.discover_product_urls(
    base_url=base_url,
    keywords=["product", "item", "å•†å“", "p/"]  # è‡ªå®šç¾©é—œéµè©
)

print(f"æ‰¾åˆ° {len(product_urls)} å€‹å•†å“é é¢")
for url in product_urls[:5]:  # é¡¯ç¤ºå‰ 5 å€‹
    print(f"  - {url}")
```

#### 1.4 é€²éšï¼šè‡ªå®šç¾©æŠ“å–é¸é …

```python
# ç›´æ¥ä½¿ç”¨åº•å±¤ scrape æ–¹æ³•
raw_data = connector.scrape_url(
    url="https://example.com/product/123",
    use_json_mode=True,  # ä½¿ç”¨ AI çµæ§‹åŒ–æå–
    wait_for=5000  # ç­‰å¾… 5 ç§’è®“é é¢å®Œå…¨è¼‰å…¥
)

# æŸ¥çœ‹åŸå§‹æ•¸æ“š
print(raw_data.keys())  # ['markdown', 'html', 'json', 'metadata']
print(raw_data['json'])  # AI æå–çš„çµæ§‹åŒ–æ•¸æ“š
```

#### 1.5 ä½¿ç”¨ Actions è™•ç†è¤‡é›œé é¢

```python
# ç¯„ä¾‹ï¼šéœ€è¦é»æ“Šã€ŒæŸ¥çœ‹è©³æƒ…ã€æŒ‰éˆ•æ‰èƒ½çœ‹åˆ°åƒ¹æ ¼
actions = [
    {"type": "wait", "milliseconds": 2000},  # ç­‰å¾…é é¢è¼‰å…¥
    {"type": "click", "selector": "button.view-details"},  # é»æ“ŠæŒ‰éˆ•
    {"type": "wait", "milliseconds": 1000},  # ç­‰å¾…å…§å®¹å‡ºç¾
    {"type": "scroll", "direction": "down", "amount": 500},  # æ»¾å‹•
]

raw_data = connector.scrape_with_actions(
    url="https://complex-site.com/product/456",
    actions=actions,
    take_screenshot=True  # å¯é¸ï¼šæˆªåœ–é©—è­‰
)
```

---

### æ–¹å¼ 2ï¼šé€é Celery ä»»å‹™ï¼ˆç•°æ­¥ï¼‰

é€™æ˜¯ç³»çµ±å…§å»ºçš„æ–¹å¼ï¼Œé©åˆå®šæ™‚æ‰¹é‡æŠ“å–ã€‚

#### 2.1 æŠ“å–å–®å€‹å•†å“

```python
from app.tasks.scrape_tasks import scrape_single_product

# æäº¤ç•°æ­¥ä»»å‹™ï¼ˆéœ€è¦å…ˆå‰µå»º CompetitorProduct è¨˜éŒ„ï¼‰
task = scrape_single_product.delay(product_id="uuid-here")

# æŸ¥çœ‹ä»»å‹™ç‹€æ…‹
print(f"ä»»å‹™ ID: {task.id}")
print(f"ä»»å‹™ç‹€æ…‹: {task.state}")

# ç­‰å¾…çµæœï¼ˆæœƒé˜»å¡ï¼‰
result = task.get(timeout=30)
print(result)
```

#### 2.2 æŠ“å–æ•´å€‹ç«¶çˆ­å°æ‰‹çš„æ‰€æœ‰å•†å“

```python
from app.tasks.scrape_tasks import scrape_competitor

# æŠ“å–æŸå€‹ç«¶çˆ­å°æ‰‹çš„æ‰€æœ‰å•†å“
competitor_id = "competitor-uuid-here"
task = scrape_competitor.delay(competitor_id)

# ç•°æ­¥åŸ·è¡Œï¼Œä¸æœƒé˜»å¡
print(f"å·²æäº¤æŠ“å–ä»»å‹™: {task.id}")
```

#### 2.3 å®šæ™‚ä»»å‹™ï¼šæ¯å¤©è‡ªå‹•æŠ“å–

```python
from app.tasks.scrape_tasks import scrape_all_competitors

# æ‰‹å‹•è§¸ç™¼æ‰€æœ‰ç«¶çˆ­å°æ‰‹çš„æŠ“å–
task = scrape_all_competitors.delay()
```

åœ¨ `backend/app/tasks/celery_app.py` ä¸­é…ç½®å®šæ™‚ä»»å‹™ï¼š

```python
from celery.schedules import crontab

app.conf.beat_schedule = {
    'scrape-competitors-daily': {
        'task': 'app.tasks.scrape_tasks.scrape_all_competitors',
        'schedule': crontab(hour=9, minute=0),  # æ¯å¤©æ—©ä¸Š 9 é»
    },
}
```

---

### æ–¹å¼ 3ï¼šé€é API ç«¯é»

å‰ç«¯å¯ä»¥é€é HTTP API è§¸ç™¼æŠ“å–ä»»å‹™ã€‚

#### 3.1 API è·¯ç”±è¨­è¨ˆï¼ˆéœ€è¦å…ˆå¯¦ç¾ï¼‰

åœ¨ `backend/app/api/v1/scraping.py` å‰µå»ºï¼š

```python
from fastapi import APIRouter, BackgroundTasks
from app.tasks.scrape_tasks import scrape_single_product

router = APIRouter()

@router.post("/scrape/product/{product_id}")
async def trigger_scrape(product_id: str, background_tasks: BackgroundTasks):
    """è§¸ç™¼å–®å€‹å•†å“æŠ“å–"""
    task = scrape_single_product.delay(product_id)
    return {
        "task_id": task.id,
        "status": "queued",
        "message": "æŠ“å–ä»»å‹™å·²æäº¤"
    }

@router.get("/scrape/status/{task_id}")
async def get_scrape_status(task_id: str):
    """æŸ¥è©¢æŠ“å–ä»»å‹™ç‹€æ…‹"""
    from celery.result import AsyncResult
    result = AsyncResult(task_id)
    return {
        "task_id": task_id,
        "state": result.state,
        "result": result.result if result.ready() else None
    }
```

#### 3.2 å‰ç«¯èª¿ç”¨ç¯„ä¾‹

```javascript
// è§¸ç™¼æŠ“å–
const response = await fetch('/api/v1/scrape/product/uuid-123', {
  method: 'POST'
});
const { task_id } = await response.json();

// è¼ªè©¢æŸ¥è©¢ç‹€æ…‹
const checkStatus = setInterval(async () => {
  const status = await fetch(`/api/v1/scrape/status/${task_id}`);
  const data = await status.json();

  if (data.state === 'SUCCESS') {
    console.log('æŠ“å–å®Œæˆ:', data.result);
    clearInterval(checkStatus);
  }
}, 2000);
```

---

## å¯¦æˆ°æ¡ˆä¾‹

### æ¡ˆä¾‹ 1ï¼šç›£æ§æ—¥æœ¬æ¨‚å¤©å•†å“åƒ¹æ ¼

```python
from app.connectors.firecrawl import get_firecrawl_connector
from decimal import Decimal

connector = get_firecrawl_connector()

# ç›®æ¨™å•†å“ URL
rakuten_url = "https://item.rakuten.co.jp/shop/item-12345/"

# æŠ“å–å•†å“è³‡è¨Š
info = connector.extract_product_info(rakuten_url)

# æª¢æŸ¥æ˜¯å¦é™åƒ¹ï¼ˆå‡è¨­æˆ‘å€‘è¨˜éŒ„äº†ä¸Šæ¬¡åƒ¹æ ¼ï¼‰
last_price = Decimal("5980")  # ä¸Šæ¬¡è¨˜éŒ„çš„åƒ¹æ ¼

if info.price and info.price < last_price:
    discount = ((last_price - info.price) / last_price * 100)
    print(f"ğŸ‰ åƒ¹æ ¼ä¸‹é™ {discount:.1f}%ï¼")
    print(f"   åŸåƒ¹: Â¥{last_price}")
    print(f"   ç¾åƒ¹: Â¥{info.price}")
else:
    print(f"åƒ¹æ ¼æœªè®Šå‹•: Â¥{info.price}")
```

### æ¡ˆä¾‹ 2ï¼šæ‰¹é‡ç›£æ§å¤šå€‹ç«¶çˆ­å°æ‰‹

```python
from app.connectors.firecrawl import get_firecrawl_connector
import time

connector = get_firecrawl_connector()

# ç«¶çˆ­å°æ‰‹å•†å“åˆ—è¡¨
competitors = {
    "ç«¶çˆ­å°æ‰‹ A": "https://shop-a.com/product/123",
    "ç«¶çˆ­å°æ‰‹ B": "https://shop-b.com/item/456",
    "ç«¶çˆ­å°æ‰‹ C": "https://shop-c.jp/goods/789",
}

print("é–‹å§‹æ‰¹é‡æŠ“å–...")
results = []

for name, url in competitors.items():
    try:
        info = connector.extract_product_info(url)
        results.append({
            "åº—å®¶": name,
            "å•†å“": info.name,
            "åƒ¹æ ¼": float(info.price) if info.price else None,
            "åº«å­˜": info.stock_status
        })
        print(f"âœ“ {name}: {info.name} - HK${info.price}")
        time.sleep(1)  # é¿å…è«‹æ±‚éå¿«
    except Exception as e:
        print(f"âœ— {name}: æŠ“å–å¤±æ•— - {e}")
        results.append({
            "åº—å®¶": name,
            "éŒ¯èª¤": str(e)
        })

# é¡¯ç¤ºå°æ¯”è¡¨æ ¼
print("\n=== åƒ¹æ ¼å°æ¯” ===")
for r in results:
    if "åƒ¹æ ¼" in r and r["åƒ¹æ ¼"]:
        print(f"{r['åº—å®¶']:15s} | HK${r['åƒ¹æ ¼']:8.2f} | {r['åº«å­˜']}")
```

### æ¡ˆä¾‹ 3ï¼šè‡ªå‹•ç™¼ç¾ä¸¦ç›£æ§æ–°å•†å“

```python
from app.connectors.firecrawl import get_firecrawl_connector
from app.models.database import async_session_maker
from app.models.competitor import Competitor, CompetitorProduct
from sqlalchemy import select

connector = get_firecrawl_connector()

async def discover_new_products(competitor_id: str):
    """ç™¼ç¾ç«¶çˆ­å°æ‰‹ç¶²ç«™ä¸Šçš„æ–°å•†å“"""

    async with async_session_maker() as db:
        # ç²å–ç«¶çˆ­å°æ‰‹è³‡è¨Š
        result = await db.execute(
            select(Competitor).where(Competitor.id == competitor_id)
        )
        competitor = result.scalar_one()

        # ç™¼ç¾æ‰€æœ‰å•†å“ URL
        print(f"æ­£åœ¨æƒæ {competitor.name} çš„ç¶²ç«™...")
        product_urls = connector.discover_product_urls(
            base_url=competitor.website_url,
            keywords=["product", "item", "å•†å“"]
        )

        print(f"ç™¼ç¾ {len(product_urls)} å€‹å•†å“é é¢")

        # æª¢æŸ¥å“ªäº›æ˜¯æ–°å•†å“
        existing_result = await db.execute(
            select(CompetitorProduct.url).where(
                CompetitorProduct.competitor_id == competitor_id
            )
        )
        existing_urls = {row[0] for row in existing_result.all()}

        new_urls = [url for url in product_urls if url not in existing_urls]
        print(f"å…¶ä¸­ {len(new_urls)} å€‹æ˜¯æ–°å•†å“")

        # ç‚ºæ–°å•†å“å‰µå»ºè¨˜éŒ„
        for url in new_urls[:10]:  # é™åˆ¶ä¸€æ¬¡æœ€å¤š 10 å€‹
            try:
                info = connector.extract_product_info(url)

                new_product = CompetitorProduct(
                    competitor_id=competitor_id,
                    name=info.name,
                    url=url,
                    sku=info.sku,
                    image_url=info.image_url,
                    is_active=True
                )
                db.add(new_product)
                print(f"  âœ“ æ–°å¢å•†å“: {info.name}")

            except Exception as e:
                print(f"  âœ— ç„¡æ³•æŠ“å– {url}: {e}")

        await db.commit()
        print(f"å®Œæˆï¼å…±æ–°å¢ {len(new_urls[:10])} å€‹å•†å“")

# ä½¿ç”¨ç¯„ä¾‹
# await discover_new_products("competitor-uuid-here")
```

### æ¡ˆä¾‹ 4ï¼šè™•ç†éœ€è¦ç™»å…¥çš„é é¢

æŸäº›å•†å“é é¢éœ€è¦ç™»å…¥æ‰èƒ½çœ‹åˆ°åƒ¹æ ¼ï¼š

```python
# ä½¿ç”¨ Actions æ¨¡æ“¬ç™»å…¥æµç¨‹
actions = [
    # 1. ç­‰å¾…é é¢è¼‰å…¥
    {"type": "wait", "milliseconds": 2000},

    # 2. é»æ“Šç™»å…¥æŒ‰éˆ•
    {"type": "click", "selector": "button.login"},

    # 3. ç­‰å¾…ç™»å…¥è¡¨å–®å‡ºç¾
    {"type": "wait", "milliseconds": 1000},

    # 4. å¡«å¯«å¸³è™Ÿå¯†ç¢¼ï¼ˆæ³¨æ„ï¼šé€™ç¨®æ–¹å¼ä¸å®‰å…¨ï¼Œåƒ…ç”¨æ–¼æ¸¬è©¦ï¼‰
    {"type": "write", "text": "test@example.com"},
    {"type": "press", "key": "Tab"},
    {"type": "write", "text": "password123"},

    # 5. æäº¤è¡¨å–®
    {"type": "click", "selector": "button[type='submit']"},

    # 6. ç­‰å¾…ç™»å…¥å®Œæˆä¸¦è·³è½‰
    {"type": "wait", "milliseconds": 3000},
]

raw_data = connector.scrape_with_actions(
    url="https://members-only-shop.com/product/123",
    actions=actions,
    take_screenshot=True  # æˆªåœ–é©—è­‰ç™»å…¥ç‹€æ…‹
)

# æ³¨æ„ï¼šæ›´å¥½çš„åšæ³•æ˜¯ä½¿ç”¨ Cookies æˆ– Session
```

---

## é€²éšæŠ€å·§

### æŠ€å·§ 1ï¼šæé«˜æŠ“å–æº–ç¢ºç‡

```python
# é‡å°ä¸åŒç¶²ç«™ä½¿ç”¨ä¸åŒçš„ Schema
class RakutenProductSchema(BaseModel):
    """æ¨‚å¤©å°ˆç”¨ Schema"""
    å•†å“å: Optional[str] = None
    ä¾¡æ ¼: Optional[float] = None
    å…ƒå€¤: Optional[float] = None
    åœ¨åº«: Optional[str] = None

# åœ¨ firecrawl.py ä¸­å¯ä»¥æ ¹æ“š URL å‹•æ…‹é¸æ“‡ Schema
def get_schema_for_url(url: str):
    if "rakuten.co.jp" in url:
        return RakutenProductSchema
    elif "amazon.co.jp" in url:
        return AmazonProductSchema
    else:
        return ProductSchema  # é è¨­
```

### æŠ€å·§ 2ï¼šè™•ç† IP å°é–

Firecrawl æœƒè‡ªå‹•è™•ç†ä»£ç†å’Œåçˆ¬èŸ²ï¼Œä½†å¦‚æœé‡åˆ°å•é¡Œï¼š

```python
# å¢åŠ ç­‰å¾…æ™‚é–“
info = connector.extract_product_info(
    url=target_url,
    use_actions=True  # æ¨¡æ“¬çœŸäººæ“ä½œ
)

# åœ¨ scrape_url ä¸­å¢åŠ  wait_for åƒæ•¸
raw_data = connector.scrape_url(
    url=target_url,
    wait_for=10000  # ç­‰å¾… 10 ç§’
)
```

### æŠ€å·§ 3ï¼šå„ªåŒ–æ‰¹é‡æŠ“å–æ€§èƒ½

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

def scrape_single(url):
    """å–®å€‹æŠ“å–å‡½æ•¸ï¼ˆåŒæ­¥ï¼‰"""
    return connector.extract_product_info(url)

# ä½¿ç”¨ç·šç¨‹æ± ä¸¦è¡ŒæŠ“å–
with ThreadPoolExecutor(max_workers=5) as executor:
    urls = ["url1", "url2", "url3", ...]
    results = list(executor.map(scrape_single, urls))

# æ³¨æ„ï¼šFirecrawl API æœ‰é€Ÿç‡é™åˆ¶ï¼Œä¸è¦ä¸¦ç™¼éå¤š
```

### æŠ€å·§ 4ï¼šéŒ¯èª¤é‡è©¦æ©Ÿåˆ¶

```python
import time
from typing import Optional

def scrape_with_retry(
    url: str,
    max_retries: int = 3,
    delay: int = 2
) -> Optional[ProductInfo]:
    """å¸¶é‡è©¦çš„æŠ“å–"""

    for attempt in range(max_retries):
        try:
            info = connector.extract_product_info(url)
            return info
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"æŠ“å–å¤±æ•—ï¼Œ{delay}ç§’å¾Œé‡è©¦... ({attempt + 1}/{max_retries})")
                time.sleep(delay)
                delay *= 2  # æŒ‡æ•¸é€€é¿
            else:
                print(f"é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œæ”¾æ£„æŠ“å–: {e}")
                return None
```

### æŠ€å·§ 5ï¼šç›£æ§ API ç”¨é‡

```python
from app.models.system import ScrapeLog
from sqlalchemy import select, func
from datetime import datetime, timedelta

async def get_api_usage_stats(db):
    """æŸ¥è©¢æœ€è¿‘ 30 å¤©çš„ API ä½¿ç”¨çµ±è¨ˆ"""

    thirty_days_ago = datetime.utcnow() - timedelta(days=30)

    result = await db.execute(
        select(
            func.count(ScrapeLog.id).label('total_scrapes'),
            func.sum(ScrapeLog.products_scraped).label('total_products'),
            func.sum(ScrapeLog.products_failed).label('total_failures')
        ).where(
            ScrapeLog.started_at >= thirty_days_ago
        )
    )

    stats = result.one()

    print(f"è¿‘ 30 å¤©çµ±è¨ˆ:")
    print(f"  ç¸½æŠ“å–æ¬¡æ•¸: {stats.total_scrapes}")
    print(f"  æˆåŠŸå•†å“æ•¸: {stats.total_products}")
    print(f"  å¤±æ•—å•†å“æ•¸: {stats.total_failures}")
    print(f"  æˆåŠŸç‡: {stats.total_products / (stats.total_products + stats.total_failures) * 100:.1f}%")
```

---

## å¸¸è¦‹å•é¡Œæ’æŸ¥

### Q1: `ValueError: Firecrawl API Key æœªè¨­å®š`

**åŸå› **ï¼šç’°å¢ƒè®Šæ•¸ä¸­æ²’æœ‰è¨­å®š API Key

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```bash
# æª¢æŸ¥ .env æ–‡ä»¶
cat backend/.env | grep FIRECRAWL

# å¦‚æœæ²’æœ‰ï¼Œæ·»åŠ ï¼š
echo "FIRECRAWL_API_KEY=fc-your-key" >> backend/.env

# é‡å•Ÿæ‡‰ç”¨
```

### Q2: `ImportError: cannot import name 'FirecrawlApp'`

**åŸå› **ï¼šfirecrawl-py å¥—ä»¶æœªæ­£ç¢ºå®‰è£

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```bash
pip uninstall firecrawl-py
pip install firecrawl-py==0.0.16

# é©—è­‰å®‰è£
python -c "from firecrawl import FirecrawlApp; print('OK')"
```

### Q3: æŠ“å–è¿”å›çš„åƒ¹æ ¼ç‚º None

**åŸå› **ï¼šç¶²ç«™çš„åƒ¹æ ¼æ ¼å¼ç„¡æ³•è¢«æ­£å‰‡è¡¨é”å¼åŒ¹é…

**è§£æ±ºæ–¹æ¡ˆ A**ï¼šä½¿ç”¨ JSON Mode
```python
# è®“ AI è‡ªå‹•è­˜åˆ¥åƒ¹æ ¼
info = connector.extract_product_info(url)  # å·²é è¨­ä½¿ç”¨ JSON Mode
```

**è§£æ±ºæ–¹æ¡ˆ B**ï¼šæŸ¥çœ‹åŸå§‹æ•¸æ“šä¸¦è‡ªå®šç¾©è§£æ
```python
raw_data = connector.scrape_url(url, use_json_mode=True)
print(raw_data['json'])  # æŸ¥çœ‹ AI æå–çš„çµæœ
print(raw_data['markdown'])  # æŸ¥çœ‹åŸå§‹å…§å®¹

# å¦‚æœéœ€è¦ï¼Œå¯ä»¥åœ¨ firecrawl.py ä¸­æ·»åŠ æ–°çš„åƒ¹æ ¼æ­£å‰‡æ¨¡å¼
```

### Q4: æŠ“å–é€Ÿåº¦å¾ˆæ…¢

**åŸå› **ï¼šFirecrawl éœ€è¦è™•ç† JavaScriptã€ä»£ç†ç­‰

**å„ªåŒ–æ–¹æ¡ˆ**ï¼š
```python
# 1. æ¸›å°‘ç­‰å¾…æ™‚é–“ï¼ˆå¦‚æœé é¢è¼‰å…¥å¿«ï¼‰
raw_data = connector.scrape_url(url, wait_for=1000)  # 1 ç§’

# 2. ä¸ä½¿ç”¨ JSON Modeï¼ˆè·³é AI è™•ç†ï¼‰
raw_data = connector.scrape_url(url, use_json_mode=False)

# 3. æ‰¹é‡æŠ“å–ä½¿ç”¨ crawl() è€Œéå¤šæ¬¡ scrape()
results = connector.crawl_products(base_url, limit=50)
```

### Q5: `429 Too Many Requests` éŒ¯èª¤

**åŸå› **ï¼šè¶…é API é€Ÿç‡é™åˆ¶

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```python
import time

for url in url_list:
    info = connector.extract_product_info(url)
    time.sleep(2)  # æ¯æ¬¡æŠ“å–é–“éš” 2 ç§’
```

### Q6: æŠ“å–çš„å•†å“åç¨±ä¸æº–ç¢º

**åŸå› **ï¼šé é¢æ¨™é¡ŒåŒ…å«äº†å¤šé¤˜è³‡è¨Šï¼ˆå¦‚ç¶²ç«™åç¨±ï¼‰

**è§£æ±ºæ–¹æ¡ˆ**ï¼šæ¸…ç†å•†å“åç¨±
```python
def clean_product_name(name: str) -> str:
    """æ¸…ç†å•†å“åç¨±"""
    # ç§»é™¤å¸¸è¦‹çš„å¾Œç¶´
    suffixes = [" - è³¼ç‰©ç¶²", " | æ¨‚å¤©å¸‚å ´", " - Amazon.jp"]
    for suffix in suffixes:
        if suffix in name:
            name = name.split(suffix)[0]
    return name.strip()

info = connector.extract_product_info(url)
clean_name = clean_product_name(info.name)
```

### Q7: å¦‚ä½•è™•ç†å¤šèªè¨€ç¶²ç«™

```python
# Firecrawl æœƒè‡ªå‹•è™•ç†å¤šèªè¨€
# ä½†ä½ å¯èƒ½éœ€è¦åœ¨ Schema ä¸­æŒ‡å®šèªè¨€

class MultiLangProductSchema(BaseModel):
    product_name_en: Optional[str] = None
    product_name_zh: Optional[str] = None
    product_name_ja: Optional[str] = None
    price: Optional[float] = None

# æˆ–è€…æ ¹æ“šç¶²ç«™ URL åˆ¤æ–·èªè¨€
def detect_language(url: str) -> str:
    if ".jp" in url:
        return "ja"
    elif ".hk" in url or ".tw" in url:
        return "zh"
    else:
        return "en"
```

### Q8: å¦‚ä½•é©—è­‰æŠ“å–çµæœæ˜¯å¦æ­£ç¢º

```python
def validate_product_info(info: ProductInfo) -> bool:
    """é©—è­‰å•†å“è³‡è¨Šå®Œæ•´æ€§"""

    issues = []

    if not info.name or info.name == "æœªçŸ¥å•†å“":
        issues.append("å•†å“åç¨±ç¼ºå¤±")

    if not info.price:
        issues.append("åƒ¹æ ¼ç¼ºå¤±")

    if info.price and info.price <= 0:
        issues.append("åƒ¹æ ¼ç„¡æ•ˆ")

    if info.discount_percent and info.discount_percent > 90:
        issues.append("æŠ˜æ‰£ç•°å¸¸ï¼ˆè¶…é 90%ï¼‰")

    if issues:
        print(f"âš ï¸  é©—è­‰è­¦å‘Š: {', '.join(issues)}")
        return False

    return True

# ä½¿ç”¨
info = connector.extract_product_info(url)
if validate_product_info(info):
    print("âœ“ è³‡æ–™é©—è­‰é€šé")
else:
    print("âœ— è³‡æ–™é©—è­‰å¤±æ•—ï¼Œå»ºè­°äººå·¥æª¢æŸ¥")
```

---

## æ¸¬è©¦èˆ‡é™¤éŒ¯

### æ¸¬è©¦è…³æœ¬ç¯„ä¾‹

å‰µå»º `backend/test_scraping.py`ï¼š

```python
"""
æŠ“å–åŠŸèƒ½æ¸¬è©¦è…³æœ¬
ç”¨æ³•: python test_scraping.py
"""

from app.connectors.firecrawl import get_firecrawl_connector
from app.config import get_settings

def test_basic_scraping():
    """æ¸¬è©¦åŸºæœ¬æŠ“å–åŠŸèƒ½"""
    print("=== æ¸¬è©¦ 1: åŸºæœ¬æŠ“å– ===")

    connector = get_firecrawl_connector()

    # ä½¿ç”¨ä¸€å€‹å…¬é–‹çš„æ¸¬è©¦å•†å“é é¢
    test_url = "https://www.example.com/product"  # æ›¿æ›ç‚ºçœŸå¯¦ URL

    try:
        info = connector.extract_product_info(test_url)

        print(f"âœ“ å•†å“åç¨±: {info.name}")
        print(f"âœ“ åƒ¹æ ¼: {info.price}")
        print(f"âœ“ åº«å­˜: {info.stock_status}")

        return True
    except Exception as e:
        print(f"âœ— æŠ“å–å¤±æ•—: {e}")
        return False

def test_url_discovery():
    """æ¸¬è©¦ URL ç™¼ç¾åŠŸèƒ½"""
    print("\n=== æ¸¬è©¦ 2: URL ç™¼ç¾ ===")

    connector = get_firecrawl_connector()
    base_url = "https://www.example.com"  # æ›¿æ›ç‚ºçœŸå¯¦ç¶²ç«™

    try:
        result = connector.map_urls(base_url, limit=10)
        print(f"âœ“ æ‰¾åˆ° {result.total} å€‹ URL")

        for url in result.urls[:3]:
            print(f"  - {url}")

        return True
    except Exception as e:
        print(f"âœ— URL ç™¼ç¾å¤±æ•—: {e}")
        return False

def test_config():
    """æ¸¬è©¦é…ç½®"""
    print("\n=== æ¸¬è©¦ 0: é…ç½®æª¢æŸ¥ ===")

    settings = get_settings()

    if not settings.firecrawl_api_key:
        print("âœ— Firecrawl API Key æœªè¨­å®š")
        return False

    print(f"âœ“ API Key: {settings.firecrawl_api_key[:10]}...")
    return True

if __name__ == "__main__":
    print("é–‹å§‹æ¸¬è©¦æŠ“å–åŠŸèƒ½...\n")

    results = []
    results.append(("é…ç½®æª¢æŸ¥", test_config()))
    results.append(("åŸºæœ¬æŠ“å–", test_basic_scraping()))
    results.append(("URL ç™¼ç¾", test_url_discovery()))

    print("\n" + "=" * 50)
    print("æ¸¬è©¦çµæœåŒ¯ç¸½:")
    for name, passed in results:
        status = "âœ“ é€šé" if passed else "âœ— å¤±æ•—"
        print(f"  {name}: {status}")

    total = len(results)
    passed = sum(1 for _, p in results if p)
    print(f"\nç¸½è¨ˆ: {passed}/{total} é€šé")
```

é‹è¡Œæ¸¬è©¦ï¼š

```bash
cd backend
python test_scraping.py
```

---

## æœ€ä½³å¯¦è¸å»ºè­°

### 1. æŠ“å–é »ç‡æ§åˆ¶

```python
# âŒ ä¸å¥½çš„åšæ³•ï¼šç„¡é™åˆ¶æŠ“å–
for url in all_urls:
    connector.extract_product_info(url)

# âœ… å¥½çš„åšæ³•ï¼šæ§åˆ¶é »ç‡
import time

for url in all_urls:
    connector.extract_product_info(url)
    time.sleep(2)  # æ¯æ¬¡é–“éš” 2 ç§’
```

### 2. éŒ¯èª¤è™•ç†

```python
# âŒ ä¸å¥½çš„åšæ³•ï¼šå¿½ç•¥éŒ¯èª¤
info = connector.extract_product_info(url)

# âœ… å¥½çš„åšæ³•ï¼šå®Œæ•´éŒ¯èª¤è™•ç†
try:
    info = connector.extract_product_info(url)
    # è™•ç†çµæœ...
except ValueError as e:
    logger.error(f"é…ç½®éŒ¯èª¤: {e}")
except ConnectionError as e:
    logger.error(f"ç¶²è·¯éŒ¯èª¤: {e}")
except Exception as e:
    logger.error(f"æœªçŸ¥éŒ¯èª¤: {e}")
    # è¨˜éŒ„åˆ°è³‡æ–™åº«
```

### 3. æ•¸æ“šé©—è­‰

```python
# âœ… æŠ“å–å¾Œé©—è­‰æ•¸æ“šåˆç†æ€§
info = connector.extract_product_info(url)

if not info.price or info.price <= 0:
    logger.warning(f"å•†å“ {url} åƒ¹æ ¼ç•°å¸¸")
    # è§¸ç™¼äººå·¥å¯©æ ¸

if info.discount_percent and info.discount_percent > 80:
    logger.info(f"ç™¼ç¾å¤§æŠ˜æ‰£å•†å“: {info.name}")
    # ç™¼é€é€šçŸ¥
```

### 4. ä½¿ç”¨æ—¥èªŒ

```python
import logging

logger = logging.getLogger(__name__)

# è¨˜éŒ„é—œéµæ“ä½œ
logger.info(f"é–‹å§‹æŠ“å–å•†å“: {url}")
info = connector.extract_product_info(url)
logger.info(f"æŠ“å–æˆåŠŸ: {info.name}, åƒ¹æ ¼: {info.price}")
```

### 5. ç›£æ§èˆ‡å‘Šè­¦

```python
# è¨­å®šæŠ“å–å¤±æ•—ç‡å‘Šè­¦
async def check_scraping_health(db):
    """æª¢æŸ¥æŠ“å–å¥åº·åº¦"""

    # æŸ¥è©¢æœ€è¿‘ 1 å°æ™‚çš„æŠ“å–çµ±è¨ˆ
    one_hour_ago = datetime.utcnow() - timedelta(hours=1)

    result = await db.execute(
        select(
            func.count(ScrapeLog.id).label('total'),
            func.sum(case((ScrapeLog.status == 'failed', 1), else_=0)).label('failed')
        ).where(
            ScrapeLog.started_at >= one_hour_ago
        )
    )

    stats = result.one()

    if stats.total > 0:
        failure_rate = stats.failed / stats.total

        if failure_rate > 0.3:  # å¤±æ•—ç‡è¶…é 30%
            # ç™¼é€å‘Šè­¦éƒµä»¶
            send_alert(f"æŠ“å–å¤±æ•—ç‡ç•°å¸¸: {failure_rate * 100:.1f}%")
```

---

## é™„éŒ„

### A. æ”¯æ´çš„ç¶²ç«™é¡å‹

Firecrawl ç†è«–ä¸Šæ”¯æ´æ‰€æœ‰ç¶²ç«™ï¼Œä½†å°ä»¥ä¸‹é¡å‹æ•ˆæœæœ€å¥½ï¼š

- âœ… é›»å•†ç¶²ç«™ï¼ˆAmazonã€æ¨‚å¤©ã€æ·˜å¯¶ç­‰ï¼‰
- âœ… æ–°èç¶²ç«™
- âœ… éƒ¨è½æ ¼
- âœ… æ–‡æª”ç¶²ç«™
- âš ï¸ éœ€è¦ç™»å…¥çš„æœƒå“¡ç¶²ç«™ï¼ˆéœ€è¦é¡å¤–é…ç½®ï¼‰
- âš ï¸ æœ‰åš´æ ¼åçˆ¬èŸ²çš„ç¶²ç«™ï¼ˆå¯èƒ½éœ€è¦å¤šæ¬¡é‡è©¦ï¼‰
- âŒ å®Œå…¨å‹•æ…‹ç”Ÿæˆä¸”ç„¡ SSR çš„ SPAï¼ˆå¯èƒ½éœ€è¦ Actionsï¼‰

### B. API é…é¡èªªæ˜

æ ¹æ“š Firecrawl æ–¹æ¡ˆä¸åŒï¼Œé…é¡é™åˆ¶ä¸åŒï¼š

| æ–¹æ¡ˆ | æ¯æœˆé…é¡ | é€Ÿç‡é™åˆ¶ |
|------|---------|---------|
| Free | 500 credits | 2 req/s |
| Starter | 5,000 credits | 10 req/s |
| Pro | 50,000 credits | 50 req/s |

æ¯æ¬¡ `scrape()` æ¶ˆè€— 1 creditï¼Œ`crawl()` æŒ‰é é¢æ•¸è¨ˆç®—ã€‚

### C. ç›¸é—œè³‡æº

- **Firecrawl å®˜æ–¹æ–‡æª”**: https://docs.firecrawl.dev
- **Python SDK**: https://github.com/mendableai/firecrawl-py
- **API åƒè€ƒ**: https://docs.firecrawl.dev/api-reference/introduction

### D. è¯çµ¡æ”¯æ´

å¦‚é‡åˆ°ç„¡æ³•è§£æ±ºçš„å•é¡Œï¼š

1. æŸ¥çœ‹ Firecrawl Dashboard çš„æ—¥èªŒ
2. æª¢æŸ¥ `backend/logs/` ä¸‹çš„éŒ¯èª¤æ—¥èªŒ
3. æŸ¥è©¢ Firecrawl Discord ç¤¾ç¾¤
4. æäº¤ GitHub Issue

---

**æ–‡æª”ç‰ˆæœ¬**: v1.0
**æœ€å¾Œæ›´æ–°**: 2026-01-06
**ç¶­è­·è€…**: GoGoJap æŠ€è¡“åœ˜éšŠ
